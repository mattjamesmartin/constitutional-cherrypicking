# Chile Processing and Analysis Code

## Requirements

- A Python 3 environment (3.9.21 recommended)
- The ability to run Jupyter notebooks.
- All packages listed in required_packages.txt

## Installation

### Obtain the repository

The recommended method is to use GitHub Desktop. The default file system comprises:

- `cython/`: Directory containing prebuilt shared objects (`angular_distance.so`) for computing angular distance in Mac Intel, Mac ARM, and Linux Intel environments. However, you may need to build a version of the shared object for your own environment even if it is one of the listed environments.
- `installer.ipynb`: Jupyter notebook for running installation of data and NLP model resources â€” see below.
- `models/`: Directory containing placeholder sub-directories. Each directory corresponds to a Chilean data source.
- `processing/`: Python code for initialising the CCP model and for processing Chilean data sources. The processing user configurable.
- `README.md`: This file.

### Run `installer.ipynb` 

This will populate the top-level directory with the following:

-  `data/`: a directory containing Chilean data sources and benchmark data.
-  `model_ccp/`: Contains serialised objects from processing of CCP constitution text and topics.
-  `use_ml_3/`: Multilingual sentence-level encoder.

**DO NOT ADD ANY INSTALLED DIRECTORIES OR FILES TO THE REPO.**

### Install the correct `angular_distance.so` file

The `processing/` directory contains `angular_distance.so` for a Mac Intel environment by default. Replace with the version for your environment which can be found in `cython` sub-directories.

If there are errors generated by a supplied `angular_distance.so` file when running processing code (see below), then you will have to compile a version for your environment. To do this follow the instructions in `cython/README.txt`. You will need to be able to compile C code, e.g., GCC on Linux, command-line tools on a Mac (run `xcode-select --install` in Terminal to install).

### Processing: Run `initialise.py`

- Switch to the `processing/` directory.
- In `initialise.py` set `spacy_config` to use the correct model for your version of `spaCy`.
- Run `initialise.py`. This will update the contents of `../model_ccp/` with resources needed for the Chilean data analysis.

### Processing: Run `pipeline.py`

`pipeline.py` processes the data sources in `../data/` and can be configured using the `config` data structure in the `main()` function. Choose which data sources to process by setting the value of a source's `run` flag to `True`. The configuration is set up to match data source directory names with placeholder sub-directories in `../models/` so you won't need to change configuration data other than the `run` flag.

- Switch to the `processing/` directory.
- In `pipeline.py` set `spacy_config` to use the correct model for your version of `spaCy`.
- Decide which data sources to process using the configuration.
- Run `pipline.py`. This will process the selected data sources and populate the corresponding sub-directories in `../models/`.

